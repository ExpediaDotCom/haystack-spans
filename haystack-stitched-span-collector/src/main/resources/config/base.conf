akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  loglevel = "INFO"

  actor {
    default-dispatcher {
      fork-join-executor {
        parallelism-min = 4
        parallelism-factor = 2.0
        parallelism-max = 8
      }
      throughput = 100
    }
  }

  kafka.consumer {
    # Tuning property of scheduled polls.
    poll-interval = 100ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that blocking of the thread that
    # is executing the stage will be blocked.
    poll-timeout = 100ms

    # The stage will be await outstanding offset commit requests before
    # shutting down, but if that takes longer than this timeout it will
    # stop forcefully.
    stop-timeout = 30s

    # How long to wait for `KafkaConsumer.close`
    close-timeout = 15s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `TimeoutException`.
    commit-timeout = 15s

    # If the KafkaConsumer can't connect to the broker the poll will be
    # aborted after this timeout. The KafkaConsumerActor will throw
    # org.apache.kafka.common.errors.WakeupException which will be ignored
    # until max-wakeups limit gets exceeded.
    wakeup-timeout = 3s

    # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
    max-wakeups = 5

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
    # can be defined in this configuration section.
    kafka-clients {
      # Disable auto-commit by default
      enable.auto.commit = false
      group.id = "stitched-span-collector"
    }
  }
}

collector {
  topic = "stitch-span"
  write.batch {
    size = 200
    timeout.ms = 500
  }
  parallelism = 2
  commit.batch.size = 1000 # recommeded to keep it as a multiple of write.batch.size
}

cassandra {
  # multiple endpoints can be provided as comma separated
  endpoints: "cassandra"

  # enable the auto.discovery mode, if true then we ignore the endpoints(above) and use auto discovery
  # mechanism to find cassandra nodes. For today we only support aws node discovery provider
  auto.discovery {
    enabled: false
#   aws: {
#      region: "us-west-2"
#      tags: {
#        name: "cassandra"
#      }
#    }
  }

  connections {
    max.per.host = 10
    read.timeout.ms = 5000
    conn.timeout.ms = 10000
    keep.alive = true
  }

  consistency.level = "one"
  ttl.sec = 86400

  keyspace: {
    # auto creates the keyspace and table name in cassandra(if absent)
    # if schema field is empty or not present, then no operation is performed
    # auto.create.schema = "CREATE KEYSPACE IF NOT EXISTS haystack WITH REPLICATION = { 'class': 'SimpleStrategy', 'replication_factor' : 1} AND durable_writes = false;\n\nuse haystack;\n\nCREATE TABLE traces (\nid varchar,\nts timestamp,\nstitchedspans blob,\nPRIMARY KEY ((id), ts)\n) WITH CLUSTERING ORDER BY (ts ASC);\n\nALTER TABLE traces WITH compaction = { 'class' :  'DateTieredCompactionStrategy'  };"

    name: "haystack"
    table.name: "traces"
  }
}

elasticsearch {
  endpoint = "http://elasticsearch:9200"
  conn.timeout.ms = 10000
  read.timeout.ms = 5000
  consistency.level = "one"

  index {
    # apply the template before starting the client, if json is empty, no operation is performed
    # template.json = "{\"template\": \"haystack-traces*\",\"settings\": {\"number_of_shards\": 1},\"mappings\": {\"spans\": {\"_source\": {\"enabled\": false},\"properties\": {\"spans\": {\"type\": \"nested\"}}}}}"

    name.prefix = "haystack-traces"
    type = "spans"
  }
}

reload {
  tables {
    index.fields.config = "indexing-fields"
  }
  config {
    endpoint = "elasticSearch"
    database.name = "reload-configs"
  }
  startup.load = true
  interval.ms = 600000 # -1 will imply 'no reload'
}